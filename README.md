# å¼€æºè¯­è¨€æ¨¡å‹ç™¾å®è¢‹ (Ver. 1.1)
Open-Source Language Model Pocket

**Github**: https://github.com/createmomo/Open-Source-Language-Model-Pocket

## 1 å·¥å…·ç®±ï¼ˆToolsï¼‰
### ColossalAI (â­18.9k)
- https://github.com/hpcaitech/ColossalAI

Colossal-AI: Making large AI models cheaper, faster and more accessible

Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.

### * ChatRWKV (â­3.4k)
- https://github.com/BlinkDL/ChatRWKV

ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)

### ChatLLaMA (â­6.7k)
- https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama

ChatLLaMA ğŸ¦™ has been designed to help developers with various use cases, all related to RLHF training and optimized inference.

ChatLLaMA is a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible. Instead of depending on one large assistant that â€œrules us allâ€, we envision a future where each of us can create our own personalized version of ChatGPT-like assistants. Imagine a future where many ChatLLaMAs at the "edge" will support a variety of human's needs. But creating a personalized assistant at the "edge" requires huge optimization efforts on many fronts: dataset creation, efficient training with RLHF, and inference optimization.

### FlexGen (â­6.8k)
- https://github.com/FMInference/FlexGen

FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.

Limitation. As an offloading-based system running on weak GPUs, FlexGen also has its limitations. FlexGen can be significantly slower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases. FlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.

### FlagAI and FlagData

- https://github.com/FlagAI-Open/FlagAI

FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.

- https://github.com/FlagOpen/FlagData

FlagData, a data processing toolkit that is easy to use and expand. FlagData integrates the tools and algorithms of multi-step data processing, including cleaning, condensation, annotation and analysis, providing powerful data processing support for model training and deployment in multiple fields, including natural language processing and computer vision. 

### Facebook LLaMA (â­11.9k)
- https://github.com/facebookresearch/llama

LLaMA: Open and Efficient Foundation Language Models

We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.

### llama.cpp (â­8.6k)
- https://github.com/ggerganov/llama.cpp

Inference of LLaMA model in pure C/C++

The main goal is to run the model using 4-bit quantization on a MacBook
- Plain C/C++ implementation without dependencies
- Apple silicon first-class citizen - optimized via ARM NEON
- AVX2 support for x86 architectures
- Mixed F16 / F32 precision
- 4-bit quantization support
- Runs on the CPU

### OpenChatKit (â­5.2k)
- https://www.together.xyz/blog/openchatkit 
- https://huggingface.co/spaces/togethercomputer/OpenChatKit
- https://github.com/togethercomputer/OpenChatKit

OpenChatKit uses a 20 billion parameter chat model trained on 43 million instructions and supports reasoning, multi-turn conversation, knowledge and generative answers.

OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai. Much more than a model release, this is the beginning of an open source project. We are releasing a set of tools and processes for ongoing improvement with community contributions.

### * Open-Assistant (â­18.9k)
- https://github.com/LAION-AI/Open-Assistant
- https://open-assistant.io/zh

Open Assistant is a project meant to give everyone access to a great chat based large language model.

We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.

### * PaLM + RLHF (Pytorch)(â­5.7k)
- https://github.com/lucidrains/PaLM-rlhf-pytorch

Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, Ã  la RETRO

### * RL4LMs (â­1.2k)
- https://github.com/allenai/RL4LMs
- https://rl4lms.apps.allenai.org/

A modular RL library to fine-tune language models to human preferences

We provide easily customizable building blocks for training language models including implementations of on-policy algorithms, reward functions, metrics, datasets and LM based actor-critic policies

### * Reinforcement Learning with Language Model
- https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¼€æºé¡¹ç›® trl æ­å»ºä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOï¼‰æ¥æ›´æ–°è¯­è¨€æ¨¡å‹ï¼ˆGPT-2ï¼‰çš„å‡ ä¸ªç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
- åŸºäºä¸­æ–‡æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ­£å‘è¯„è®ºç”Ÿæˆæœºå™¨äººï¼ˆNo Human Rewardï¼‰
- åŸºäºäººå·¥æ‰“åˆ†çš„æ­£å‘è¯„è®ºç”Ÿæˆæœºå™¨äººï¼ˆWith Human Rewardï¼‰
- åŸºäºæ’åºåºåˆ—ï¼ˆRank Listï¼‰è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰
- æ’åºåºåˆ—ï¼ˆRank Listï¼‰æ ‡æ³¨å¹³å°

### Stanford Alpaca (â­7.9k)
- https://crfm.stanford.edu/2023/03/13/alpaca.html
- https://alpaca-ai.ngrok.io/
- https://github.com/tatsu-lab/stanford_alpaca

Alpaca: A Strong, Replicable Instruction-Following ModelAl

We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAIâ€™s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$).

### * Transformer Reinforcement Learning (â­2.2k)
- https://github.com/lvwerra/trl

With trl you can train transformer language models with Proximal Policy Optimization (PPO). The library is built on top of the transformers library by ğŸ¤— Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point most of decoder architectures and encoder-decoder architectures are supported.

### * Transformer Reinforcement Learning X (â­2.5k)
- https://github.com/CarperAI/trlx

trlX is a distributed training framework designed from the ground up to focus on fine-tuning large language models with reinforcement learning using either a provided reward function or a reward-labeled dataset.

Training support for ğŸ¤— Hugging Face models is provided by Accelerate-backed trainers, allowing users to fine-tune causal and T5-based language models of up to 20B parameters, such as facebook/opt-6.7b, EleutherAI/gpt-neox-20b, and google/flan-t5-xxl. For models beyond 20B parameters, trlX provides NVIDIA NeMo-backed trainers that leverage efficient parallelism techniques to scale effectively.

## 2 ä¸­æ–‡å¼€æºæ¨¡å‹ï¼ˆChinese Open Source Language Modelsï¼‰
### ChatYuan
- https://github.com/clue-ai/ChatYuan

å…ƒè¯­åŠŸèƒ½å‹å¯¹è¯å¤§æ¨¡å‹, è¿™ä¸ªæ¨¡å‹å¯ä»¥ç”¨äºé—®ç­”ã€ç»“åˆä¸Šä¸‹æ–‡åšå¯¹è¯ã€åšå„ç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ›æ„æ€§å†™ä½œï¼Œä¹Ÿèƒ½å›ç­”ä¸€äº›åƒæ³•å¾‹ã€æ–°å† ç­‰é¢†åŸŸé—®é¢˜ã€‚å®ƒåŸºäºPromptCLUE-largeç»“åˆæ•°äº¿æ¡åŠŸèƒ½å¯¹è¯å¤šè½®å¯¹è¯æ•°æ®è¿›ä¸€æ­¥è®­ç»ƒå¾—åˆ°ã€‚

PromptCLUE-largeåœ¨1000äº¿tokenä¸­æ–‡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç´¯è®¡å­¦ä¹ 1.5ä¸‡äº¿ä¸­æ–‡tokenï¼Œå¹¶ä¸”åœ¨æ•°ç™¾ç§ä»»åŠ¡ä¸Šè¿›è¡ŒPromptä»»åŠ¡å¼è®­ç»ƒã€‚é’ˆå¯¹ç†è§£ç±»ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æŠ½å–ç­‰ï¼Œå¯ä»¥è‡ªå®šä¹‰æ ‡ç­¾ä½“ç³»ï¼›é’ˆå¯¹å¤šç§ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è¿›è¡Œé‡‡æ ·è‡ªç”±ç”Ÿæˆã€‚

### PromptCLUE
- https://github.com/clue-ai/PromptCLUE

PromptCLUEï¼šå¤§è§„æ¨¡å¤šä»»åŠ¡Prompté¢„è®­ç»ƒä¸­æ–‡å¼€æºæ¨¡å‹ã€‚

ä¸­æ–‡ä¸Šçš„ä¸‰å¤§ç»Ÿä¸€ï¼šç»Ÿä¸€æ¨¡å‹æ¡†æ¶ï¼Œç»Ÿä¸€ä»»åŠ¡å½¢å¼ï¼Œç»Ÿä¸€åº”ç”¨æ–¹å¼ã€‚

æ”¯æŒå‡ åä¸ªä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œå…·æœ‰è¾ƒå¥½çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚é’ˆå¯¹ç†è§£ç±»ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æŠ½å–ç­‰ï¼Œå¯ä»¥è‡ªå®šä¹‰æ ‡ç­¾ä½“ç³»ï¼›é’ˆå¯¹ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è¿›è¡Œé‡‡æ ·è‡ªç”±ç”Ÿæˆã€‚

åƒäº¿ä¸­æ–‡tokenä¸Šå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç´¯è®¡å­¦ä¹ 1.5ä¸‡äº¿ä¸­æ–‡tokenï¼Œäº¿çº§ä¸­æ–‡ä»»åŠ¡æ•°æ®ä¸Šå®Œæˆè®­ç»ƒï¼Œè®­ç»ƒä»»åŠ¡è¶…è¿‡150+ã€‚æ¯”baseç‰ˆå¹³å‡ä»»åŠ¡æå‡7ä¸ªç‚¹+ï¼›å…·æœ‰æ›´å¥½çš„ç†è§£ã€ç”Ÿæˆå’ŒæŠ½å–èƒ½åŠ›ï¼Œå¹¶ä¸”æ”¯æŒæ–‡æœ¬æ”¹å†™ã€çº é”™ã€çŸ¥è¯†å›¾è°±é—®ç­”ã€‚

> æŒç»­æ›´æ–°ä¸­ (Continuously Updated)... 

