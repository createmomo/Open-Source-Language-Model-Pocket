# ÂºÄÊ∫êËØ≠Ë®ÄÊ®°ÂûãÁôæÂÆùË¢ã (Ver. 1.1)
Open-Source Language Model Pocket

**Github**: https://github.com/createmomo/Open-Source-Language-Model-Pocket

## ColossalAI (‚≠ê18.8k)
https://github.com/hpcaitech/ColossalAI

**Colossal-AI: Making large AI models cheaper, faster and more accessible**

Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.

## ChatRWKV* (‚≠ê3.4k)
https://github.com/BlinkDL/ChatRWKV

**ChatRWKV** is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)

## ChatLLaMA (‚≠ê6.7k)
https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama

**ChatLLaMA** ü¶ô has been designed to help developers with various use cases, all related to RLHF training and optimized inference.

ChatLLaMA is a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible. Instead of depending on one large assistant that ‚Äúrules us all‚Äù, we envision a future where each of us can create our own personalized version of ChatGPT-like assistants. Imagine a future where many ChatLLaMAs at the "edge" will support a variety of human's needs. But creating a personalized assistant at the "edge" requires huge optimization efforts on many fronts: dataset creation, efficient training with RLHF, and inference optimization.

## FlexGen (‚≠ê6.8k)
https://github.com/FMInference/FlexGen

**FlexGen** is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.

*Limitation.* As an offloading-based system running on weak GPUs, FlexGen also has its limitations. FlexGen can be significantly slower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases. FlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.

## FlagAI and FlagData

https://github.com/FlagAI-Open/FlagAI

**FlagAI** (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.

https://github.com/FlagOpen/FlagData

**FlagData**, a data processing toolkit that is easy to use and expand. FlagData integrates the tools and algorithms of multi-step data processing, including cleaning, condensation, annotation and analysis, providing powerful data processing support for model training and deployment in multiple fields, including natural language processing and computer vision. 

## Facebook LLaMA (‚≠ê11.9k)
https://github.com/facebookresearch/llama

**LLaMA: Open and Efficient Foundation Language Models**

We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.

## llama.cpp (‚≠ê8.6k)
https://github.com/ggerganov/llama.cpp

**Inference of LLaMA model in pure C/C++**

The main goal is to run the model using 4-bit quantization on a MacBook
- Plain C/C++ implementation without dependencies
- Apple silicon first-class citizen - optimized via ARM NEON
- AVX2 support for x86 architectures
- Mixed F16 / F32 precision
- 4-bit quantization support
- Runs on the CPU

## OpenChatKit (‚≠ê5.2k)
- https://www.together.xyz/blog/openchatkit 
- https://huggingface.co/spaces/togethercomputer/OpenChatKit
- https://github.com/togethercomputer/OpenChatKit

**OpenChatKit** uses a 20 billion parameter chat model trained on 43 million instructions and supports reasoning, multi-turn conversation, knowledge and generative answers.

OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai. Much more than a model release, this is the beginning of an open source project. We are releasing a set of tools and processes for ongoing improvement with community contributions.

## Open-Assistant (‚≠ê18.9k) *
- https://github.com/LAION-AI/Open-Assistant
- https://open-assistant.io/zh

**Open Assistant** is a project meant to give everyone access to a great chat based large language model.

We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.

## PaLM + RLHF (Pytorch)(‚≠ê5.7k)*
https://github.com/lucidrains/PaLM-rlhf-pytorch

Implementation of **RLHF** (Reinforcement Learning with Human Feedback) on top of the **PaLM** architecture. Maybe I'll add retrieval functionality too, √† la RETRO

## Stanford Alpaca (‚≠ê7.9k)
- https://crfm.stanford.edu/2023/03/13/alpaca.html
- https://alpaca-ai.ngrok.io/
- https://github.com/tatsu-lab/stanford_alpaca

**Alpaca: A Strong, Replicable Instruction-Following ModelAl**

We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI‚Äôs text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$).

> ÊåÅÁª≠Êõ¥Êñ∞‰∏≠ (Continuously Updated)... 

