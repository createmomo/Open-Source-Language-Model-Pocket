# å¼€æºè¯­è¨€æ¨¡å‹ç™¾å®è¢‹ (Ver. 1.6)
Open-Source Language Model Pocket

**Github**: https://github.com/createmomo/Open-Source-Language-Model-Pocket

ç›¸å…³æ–‡ç« ï¼š
- [ç»ƒä¹ åœºï¼šç©·ç©·ç©·å­©å­å¦‚ä½•ä½“éªŒColossalAI SFTï¼ˆKaggleç¯‡ï¼‰](https://mp.weixin.qq.com/s/Q29uSNxvPMy0rC-QxHiGZA)
- [ç»ƒä¹ åœºï¼šç©·ç©·ç©·å­©å­å¦‚ä½•ä½“éªŒColossalAI SFTï¼ˆColabç¯‡ï¼‰](https://mp.weixin.qq.com/s/NS4yySeYd7QUYb7CB9V0lA)

## 1 å·¥å…·ç®±ï¼ˆToolsï¼‰
### é«˜æ•ˆå¯¹é½ç®—æ³•RAFTã€Œæœ¨ç­ã€
- https://github.com/OptimalScale/LMFlow
- https://arxiv.org/abs/2304.06767
- https://optimalscale.github.io/LMFlow/examples/raft.html

An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.

### Alpaca-LoRA
- https://github.com/tloen/alpaca-lora

Low-Rank LLaMA Instruct-Tuning

This repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA). We provide an Instruct model of similar quality to text-davinci-003 that can run on a Raspberry Pi (for research), and the code can be easily extended to the 13b, 30b, and 65b models.

In addition to the training code, which runs within five hours on a single RTX 4090, we publish a script for downloading and inference on the foundation model and LoRA, as well as the resulting LoRA weights themselves. To fine-tune cheaply and efficiently, we use Hugging Face's PEFT as well as Tim Dettmers' bitsandbytes.

Without hyperparameter tuning or validation-based checkpointing, the LoRA model produces outputs comparable to the Stanford Alpaca model. (Please see the outputs included below.) Further tuning might be able to achieve better performance; I invite interested users to give it a try and report their results.

### Alpaca-CoT
- https://github.com/PhoebusSi/Alpaca-CoT
- https://mp.weixin.qq.com/s/Q5Q3RpQ80XmpbfhSxq2R1Q

An Instruction Fine-Tuning Platform with Instruction Data Collection and Unified Large Language Models Interface

Alpaca-CoTé¡¹ç›®æ—¨åœ¨æ¢ç©¶å¦‚ä½•æ›´å¥½åœ°é€šè¿‡instruction-tuningçš„æ–¹å¼æ¥è¯±å¯¼LLMå…·å¤‡ç±»ä¼¼ChatGPTçš„äº¤äº’å’Œinstruction-followingèƒ½åŠ›ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¹¿æ³›æ”¶é›†äº†ä¸åŒç±»å‹çš„instructionï¼ˆå°¤å…¶æ˜¯Chain-of-Thoughtæ•°æ®é›†ï¼‰ï¼Œå¹¶åŸºäºLLaMAç»™å‡ºäº†æ·±å…¥ç»†è‡´çš„å®è¯ç ”ç©¶ï¼Œä»¥ä¾›æœªæ¥å·¥ä½œå‚è€ƒã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬æ˜¯é¦–ä¸ªå°†CoTæ‹“å±•è¿›Alpacaçš„å·¥ä½œï¼Œå› æ­¤ç®€ç§°ä¸º"Alpaca-CoT"ã€‚

### Auto-GPT
- https://github.com/torantulino/auto-gpt

Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM "thoughts", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.

### AlpacaFarm
- https://mp.weixin.qq.com/s/CIF2F5Vx_RSN1-LwU_ppOQ
- https://tatsu-lab.github.io/alpaca_farm_paper.pdf
- https://github.com/tatsu-lab/alpaca_farm

ä¸»æµçš„å¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒéƒ½ç¦»ä¸å¼€RLHF(äººå·¥åé¦ˆå¼ºåŒ–å­¦ä¹ )ï¼Œå…¶ä¸»è¦æ€æƒ³æ˜¯ä½¿ç”¨äººç±»ä¸“å®¶æä¾›çš„åé¦ˆç¤ºä¾‹æ¥æŒ‡å¯¼æ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå®ƒå¯ä»¥åŠ é€Ÿå¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ï¼Œæé«˜å¤§æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†ã€Œç›®å‰RLHFè¿™ä¸ªè¿‡ç¨‹æ—¢å¤æ‚åˆæ˜‚è´µã€ã€‚

â€ƒé’ˆå¯¹RLHFè¿™ä¸ªé—®é¢˜ï¼Œå­¦æœ¯ç•Œç›®å‰ä¸»è¦æœ‰ä¸¤ç§è§£å†³æ–¹æ³•ï¼šã€Œ1ï¼‰é¿å¼€RLHFã€ï¼Œæ¯”å¦‚Metaæœ€è¿‘ç ”ç©¶çš„â€œMetaæœ€æ–°æ¨¡å‹ï¼šLIMA-65Bï¼Œæ²¡æœ‰RLHFï¼Œæ¨¡å‹æ•ˆæœè¿œèƒœAlpacaï¼ï¼â€ï¼ŒéªŒè¯äº†ç²¾å¿ƒåˆ¶ä½œçš„å°‘é‡æ ‡æ³¨æ•°æ®åŒæ ·èƒ½è¾¾åˆ°ä¸é”™çš„æ•ˆæœã€‚2ï¼‰ã€Œç®€åŒ–RLHFã€ï¼Œå°±æ˜¯ä»Šå¤©ç»™å¤§å®¶åˆ†äº«çš„è¿™ç¯‡æ–‡ç« ï¼šæ–¯å¦ç¦å‘å¸ƒäº†ä¸€ä¸ªåä¸ºAlpacaFarmï¼ˆç¾Šé©¼å†œåœºï¼‰çš„æ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨é™ä½è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æˆæœ¬ï¼Œä¸”æ¯”äººå·¥æˆæœ¬ä½45å€ï¼Œå¹¶è¡¨ç°å‡ºä¸äººç±»åé¦ˆçš„é«˜åº¦ä¸€è‡´æ€§ï¼ŒåŒæ—¶ä¹Ÿä¸ºRLHFçš„ç ”ç©¶å¼€è¾Ÿäº†æ–°çš„é“è·¯ã€‚

### BELLE: Bloom-Enhanced Large Language model Engine
- https://github.com/LianjiaTech/BELLE
- https://zhuanlan.zhihu.com/p/616079388

æœ¬é¡¹ç›®ç›®æ ‡æ˜¯ä¿ƒè¿›ä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹å¼€æºç¤¾åŒºçš„å‘å±•ï¼Œæ„¿æ™¯åšèƒ½å¸®åˆ°æ¯ä¸€ä¸ªäººçš„LLM Engineã€‚ç°é˜¶æ®µæœ¬é¡¹ç›®åŸºäºä¸€äº›å¼€æºé¢„è®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚BLOOMï¼‰ï¼Œé’ˆå¯¹ä¸­æ–‡åšäº†ä¼˜åŒ–ï¼Œæ¨¡å‹è°ƒä¼˜ä»…ä½¿ç”¨ç”±ChatGPTç”Ÿäº§çš„æ•°æ®ï¼ˆä¸åŒ…å«ä»»ä½•å…¶ä»–æ•°æ®ï¼‰ã€‚

### ColossalAI
- https://github.com/hpcaitech/ColossalAI

Colossal-AI: Making large AI models cheaper, faster and more accessible

Colossal-AI provides a collection of parallel components for you. We aim to support you to write your distributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart distributed training and inference in a few lines.

### Cerebras
- https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/
- https://huggingface.co/cerebras

å¼€æº7ä¸ªå¯å•†ç”¨GPTæ¨¡å‹ï¼Œå«æ•°æ®é›†å’Œå¯ç›´æ¥ä¸‹è½½çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡: Cerebras å¼€æº 7 ä¸ª GPT æ¨¡å‹ï¼Œå‡å¯å•†ç”¨ï¼Œå‚æ•°é‡åˆ†åˆ«è¾¾åˆ° 1.11 äº¿ã€2.56 äº¿ã€5.9 äº¿ã€13 äº¿ã€27 äº¿ã€67 äº¿å’Œ 130 äº¿ã€‚å…¶ä¸­æœ€å¤§çš„æ¨¡å‹å‚æ•°é‡è¾¾åˆ° 130 äº¿ï¼Œä¸ Meta æœ€è¿‘å¼€æºçš„ LLaMA-13B ç›¸å½“ã€‚è¯¥é¡¹ç›®å¼€æºæ•°æ®é›†å’Œé¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œå…¶ä¸­é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶å¤§å°è¿‘50Gå¯ç›´æ¥ä¸‹è½½ï¼Œå¹¶ä¸”å¯ç”¨äºå•†ä¸šå’Œç ”ç©¶ç”¨é€”ã€‚ä¸æ­¤å‰çš„ GPT-3 æ¨¡å‹ç›¸æ¯”ï¼ŒCerebras å¼€æºçš„æ¨¡å‹å…·æœ‰æ›´é«˜çš„å¯ç”¨æ€§å’Œé€æ˜åº¦ï¼Œç ”ç©¶äººå‘˜å’Œå¼€å‘è€…å¯ä»¥ä½¿ç”¨å°‘é‡æ•°æ®å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œæ„å»ºå‡ºé«˜è´¨é‡çš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨ã€‚

### ChatPiXiu
- https://github.com/catqaq/ChatPiXiu

æˆ‘ä»¬æ˜¯ç¾¡é±¼æ™ºèƒ½ã€xianyu.aiã€‘ï¼Œä¸»è¦æˆå‘˜æ˜¯ä¸€ç¾¤æ¥è‡ªè€å’Œå±±ä¸‹ã€è¥¿æ¹–è¾¹ä¸Šçš„å’¸é±¼ä»¬ï¼Œå¡˜ä¸»å«ä½œç¾¡é±¼ï¼Œæƒ³åœ¨LLMsæ—¶ä»£åšç‚¹æœ‰æ„ä¹‰çš„äº‹ï¼æˆ‘ä»¬çš„å£å·æ˜¯ï¼šåšOpenNLPå’ŒOpenXï¼å¸Œæœ›åœ¨CloseAIå·æ­»æˆ‘ä»¬ä¹‹å‰é€€å‡ºæ±Ÿæ¹–ï¼

ä¹Ÿè®¸æœ‰ä¸€å¤©ï¼Œç­‰åˆ°GPT-Xå‘å¸ƒçš„æ—¶å€™ï¼Œæœ‰äººä¼šè¯´NLPä¸å­˜åœ¨äº†ï¼Œä½†æ˜¯æˆ‘ä»¬æƒ³è¯æ˜æœ‰äººæ›¾ç»æ¥è¿‡ã€çƒ­çˆ±è¿‡ï¼åœ¨ä»¥ChatGPT/GPT4ä¸ºä»£è¡¨çš„LLMsæ—¶ä»£ï¼Œåœ¨è¢«CloseAIå·æ­»ä¹‹å‰ï¼Œæˆ‘ä»¬å‘èµ·äº†OpenNLPè®¡åˆ’ï¼Œå®—æ—¨æ˜¯OpenNLP for everyone!

ChatPiXiué¡¹ç›®ä¸ºOpenNLPè®¡åˆ’çš„ç¬¬2ä¸ªæ­£å¼çš„å¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨Open ChatGPT for everyoneï¼åœ¨ä»¥ChatGPT/GPT4ä¸ºä»£è¡¨çš„LLMsæ—¶ä»£ï¼Œåœ¨è¢«OpenAIå·æ­»ä¹‹å‰ï¼Œåšä¸€ç‚¹æœ‰æ„ä¹‰çš„äº‹æƒ…ï¼æœªæ¥æœ‰ä¸€å¤©ï¼Œç­‰åˆ°GPT-Xå‘å¸ƒçš„æ—¶å€™ï¼Œæˆ–è®¸æœ‰äººä¼šè¯´NLPä¸å­˜åœ¨äº†ï¼Œä½†æ˜¯æˆ‘ä»¬æƒ³è¯æ˜æœ‰äººæ›¾æ¥è¿‡ï¼

### ChatRWKV
- https://github.com/BlinkDL/ChatRWKV

ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model, which is the only RNN (as of now) that can match transformers in quality and scaling, while being faster and saves VRAM. Training sponsored by Stability EleutherAI :)

### ChatLLaMA
- https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama

ChatLLaMA ğŸ¦™ has been designed to help developers with various use cases, all related to RLHF training and optimized inference.

ChatLLaMA is a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible. Instead of depending on one large assistant that â€œrules us allâ€, we envision a future where each of us can create our own personalized version of ChatGPT-like assistants. Imagine a future where many ChatLLaMAs at the "edge" will support a variety of human's needs. But creating a personalized assistant at the "edge" requires huge optimization efforts on many fronts: dataset creation, efficient training with RLHF, and inference optimization.

### DeepSpeed-Chat
- https://mp.weixin.qq.com/s/t3HA4Hu61LLDC3h2Njmo_Q
- https://github.com/microsoft/DeepSpeed

å¾®è½¯å®£å¸ƒå¼€æº DeepSpeed-Chatï¼Œå¸®åŠ©ç”¨æˆ·è½»æ¾è®­ç»ƒç±» ChatGPT ç­‰å¤§è¯­è¨€æ¨¡å‹ã€‚

æ®æ‚‰ï¼ŒDeep Speed Chat æ˜¯åŸºäºå¾®è½¯ Deep Speed æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“å¼€å‘è€Œæˆï¼Œå…·å¤‡è®­ç»ƒã€å¼ºåŒ–æ¨ç†ç­‰åŠŸèƒ½ï¼Œè¿˜ä½¿ç”¨äº† RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰æŠ€æœ¯ï¼Œå¯å°†è®­ç»ƒé€Ÿåº¦æå‡ 15 å€ä»¥ä¸Šï¼Œè€Œæˆæœ¬å´å¤§å¤§é™ä½ã€‚

### Dolly 1&2
- https://github.com/databrickslabs/dolly
- https://huggingface.co/databricks/dolly-v2-12b
- https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html

We show that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in 30 minutes on one machine, using high-quality training data. Surprisingly, instruction-following does not seem to require the latest or largest models: our model is only 6 billion parameters, compared to 175 billion for GPT-3. We open source the code for our model (Dolly) and show how it can be re-created on Databricks. We believe models like Dolly will help democratize LLMs, transforming them from something very few companies can afford into a commodity every company can own and customize to improve their products.

### FlexGen
- https://github.com/FMInference/FlexGen

FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.

Limitation. As an offloading-based system running on weak GPUs, FlexGen also has its limitations. FlexGen can be significantly slower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases. FlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.

### FlagAI and FlagData

- https://github.com/FlagAI-Open/FlagAI

FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.

- https://github.com/FlagOpen/FlagData

FlagData, a data processing toolkit that is easy to use and expand. FlagData integrates the tools and algorithms of multi-step data processing, including cleaning, condensation, annotation and analysis, providing powerful data processing support for model training and deployment in multiple fields, including natural language processing and computer vision. 

### Facebook LLaMA
- https://github.com/facebookresearch/llama

LLaMA: Open and Efficient Foundation Language Models

We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.

### * ã€Gorillaã€‘
- https://mp.weixin.qq.com/s/p9tx3q3Lpr4fNqdyxWhzyA
- gorilla.cs.berkeley.edu
- arxiv.org/abs/2305.15334
- https://github.com/ShishirPatil/gorilla/

å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½å¼ºå¤§ï¼Œä½†ä¸ºäº†æ›´å¥½åœ°ç”¨äºè§£å†³å®é™…é—®é¢˜ï¼Œå„å¼å„æ ·çš„ API æ˜¯å¿…ä¸å¯å°‘çš„ã€‚

åŠ åˆ©ç¦å°¼äºšå¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡å’Œå¾®è½¯ç ”ç©¶é™¢é€ å‡ºäº†ä¸€åªã€Œå¤§çŒ©çŒ©ã€Gorillaï¼Œè¯¥æ¨¡å‹èƒ½æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€ä¸ºç”¨æˆ·é€‰æ‹©åˆé€‚çš„ API æ¥æ‰§è¡Œå¯¹åº”ä»»åŠ¡ã€‚ç†è®ºä¸Šè®²ï¼Œè¿™ä¸ªæ¨¡å‹å¯ä»¥æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒç”¨å…¶å®ƒå„ç§ AI æ¨¡å‹ï¼Œå› æ­¤ Gorilla æœ‰æœ›æˆä¸ºä¸€ä¸ªç»Ÿå¾¡å…¶å®ƒ AI çš„ AI æ¨¡å‹ã€‚è¯¥é¡¹ç›®çš„ä»£ç ã€æ¨¡å‹ã€æ•°æ®å’Œæ¼”ç¤ºéƒ½å·²å‘å¸ƒã€‚

### GPT4All
- https://github.com/nomic-ai/gpt4all

Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa

### HuggingChat
- https://huggingface.co/chat/

Making the community's best AI chat models available to everyone.

### HuggingGPT
- https://mp.weixin.qq.com/s/o51CmLt2JViJ4nsKfBJfwg
- https://arxiv.org/pdf/2303.17580.pdf

HuggingGPTåˆ©ç”¨ChatGPTä½œä¸ºæ§åˆ¶å™¨ï¼Œè¿æ¥HuggingFaceç¤¾åŒºä¸­çš„å„ç§AIæ¨¡å‹ï¼Œæ¥å®Œæˆå¤šæ¨¡æ€å¤æ‚ä»»åŠ¡ã€‚

è¿™æ„å‘³ç€ï¼Œä½ å°†æ‹¥æœ‰ä¸€ç§è¶…é­”æ³•ï¼Œé€šè¿‡HuggingGPTï¼Œä¾¿å¯æ‹¥æœ‰å¤šæ¨¡æ€èƒ½åŠ›ï¼Œæ–‡ç”Ÿå›¾ã€æ–‡ç”Ÿè§†é¢‘ã€è¯­éŸ³å…¨èƒ½æ‹¿æäº†ã€‚

### HugNLP
- https://mp.weixin.qq.com/s/IpgOQJ8vrIvnjdrmGCT2FA
- https://github.com/HugAILab/HugNLP
- https://arxiv.org/abs/2302.14286

åå¸ˆå¤§HugAILabå›¢é˜Ÿç ”å‘äº†HugNLPæ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªé¢å‘ç ”ç©¶è€…å’Œå¼€å‘è€…çš„å…¨é¢ç»Ÿä¸€çš„NLPè®­ç»ƒæ¡†æ¶ï¼Œå¯æ”¯æŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æ–‡æœ¬åŒ¹é…ã€é—®ç­”ã€ä¿¡æ¯æŠ½å–ã€æ–‡æœ¬ç”Ÿæˆã€å°æ ·æœ¬å­¦ä¹ ç­‰å¤šç§NLPä»»åŠ¡æ¨¡å‹æ­å»ºå’Œè®­ç»ƒã€‚

HugNLPè¿˜é›†æˆäº†å¤§é‡æœ€æ–°çš„PromptæŠ€æœ¯ï¼Œä¾‹å¦‚Prompt-Tuningã€In-Context Learningã€Instruction-tuningï¼Œæœªæ¥è¿˜å°†å¼•å…¥Chain-of-thought

HugAILabå›¢é˜Ÿè¿˜ç ”å‘äº†ä¸€ç³»åˆ—çš„åº”ç”¨ï¼Œä¾‹å¦‚CLUE&GLUEåˆ·æ¦œå·¥å…·ï¼Œå¯æ”¯æŒChatGPTç±»æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²äº§å“HugChatï¼Œä»¥åŠç»Ÿä¸€ä¿¡æ¯æŠ½å–äº§å“HugIEç­‰ã€‚

HugNLPæ˜¯ä¸€ä¸ªåˆ†å±‚å¼æ¡†æ¶ï¼Œéµå¾ªâ€œé«˜å†…èšä½è€¦åˆâ€çš„å¼€å‘æ¨¡å¼ï¼Œå…¶æ ¸å¿ƒåŒ…æ‹¬æ¨¡å‹å±‚ï¼ˆModelsï¼‰ã€å¤„ç†å™¨å±‚ï¼ˆProcessorsï¼‰ã€è¯„ä¼°å™¨å±‚ï¼ˆEvaluatorsï¼‰å’Œåº”ç”¨å±‚ï¼ˆApplicationsï¼‰å››éƒ¨åˆ†ã€‚

### Koala: A Dialogue Model for Academic Research
- https://bair.berkeley.edu/blog/2023/04/03/koala/

In this post, we introduce Koala, a chatbot trained by fine-tuning Metaâ€™s LLaMA on dialogue data gathered from the web. We describe the dataset curation and training process of our model, and also present the results of a user study that compares our model to ChatGPT and Stanfordâ€™s Alpaca. Our results show that Koala can effectively respond to a variety of user queries, generating responses that are often preferred over Alpaca, and at least tied with ChatGPT in over half of the cases.

### LLaMAå¤åˆ»ç‰ˆOpenLLaMA
- https://github.com/openlm-research/open_llama

In this repo, we release a permissively licensed open source reproduction of Meta AI's LLaMA large language model. In this release, we're releasing a public preview of the 7B OpenLLaMA model that has been trained with 200 billion tokens. We provide PyTorch and Jax weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Stay tuned for our updates.

### LLMPrunerï¼šå¤§è¯­è¨€æ¨¡å‹è£å‰ªå·¥å…·
- https://mp.weixin.qq.com/s/u0UcCxzJOkF4fO_JI6ToQA
- https://github.com/yangjianxin1/LLMPruner

åœ¨è®¸å¤šä¸‹æ¸¸ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€åªéœ€è¦ä½¿ç”¨åˆ°ä¸€ä¸¤ç§è¯­è¨€ï¼Œä¾‹å¦‚åœ¨ä¸­æ–‡åœºæ™¯ä¸­ï¼Œä¸€èˆ¬åªä¼šç”¨åˆ°ä¸­è‹±æ–‡ã€‚ æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¯¹å¤§è¯­è¨€æ¨¡å‹çš„è¯è¡¨è¿›è¡Œè£å‰ªï¼Œåªç•™ä¸‹æ‰€éœ€çš„éƒ¨åˆ†è¯è¡¨ï¼Œè¿™æ ·ä¸ä»…èƒ½å¤Ÿå……åˆ†ä¿ç•™æ¨¡å‹çš„é¢„è®­ç»ƒçŸ¥è¯†ï¼Œå¹¶ä¸”å‡å°‘æ¨¡å‹å‚æ•°é‡ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œæå‡è®­ç»ƒé€Ÿåº¦ï¼Œä½¿ç”¨æ›´å°‘çš„æ˜¾å¡è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„finetuneè®­ç»ƒã€‚

åŸºäºä¸Šè¿°åŸå› ï¼Œç¬”è€…å¼€å‘äº†LLMPruneré¡¹ç›®ï¼Œç›®å‰ä¸»è¦åŒ…å«è£å‰ªåçš„å„ç§å‚æ•°è§„æ¨¡çš„Bloomæ¨¡å‹ã€‚å¯¹Bloomè¿›è¡Œè¯è¡¨è£å‰ªï¼Œä¿ç•™å¸¸ç”¨çš„ä¸­è‹±æ–‡tokenï¼Œè¯è¡¨ç”±250880å°†è‡³46145ï¼Œç¼©å‡ä¸ºåŸæ¥çš„18.39%ã€‚

### llama.cpp
- https://github.com/ggerganov/llama.cpp

Inference of LLaMA model in pure C/C++

The main goal is to run the model using 4-bit quantization on a MacBook
- Plain C/C++ implementation without dependencies
- Apple silicon first-class citizen - optimized via ARM NEON
- AVX2 support for x86 architectures
- Mixed F16 / F32 precision
- 4-bit quantization support
- Runs on the CPU

### Llama-X: Open Academic Research on Improving LLaMA to SOTA LLM
- https://github.com/AetherCortex/Llama-X

This is the repo for the Llama-X, which aims to:
- Progressively improve the performance of LLaMA to SOTA LLM with open-source community.
- Conduct Llama-X as an open academic research which is long-term, systematic and rigorous.
- Save the repetitive work of community and we work together to create more and faster increment.

### Lit-LLaMA ï¸
- https://github.com/Lightning-AI/lit-llama

Lit-LLaMA is:
- Simple: Single-file implementation without boilerplate.
- Correct: Numerically equivalent to the original model.
- Optimized: Runs on consumer hardware or at scale.
- Open-source: No strings attached.

### MLC LLM
- https://github.com/mlc-ai/mlc-llm

MLC LLM is a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and native applications, plus a productive framework for everyone to further optimize model performance for their own use cases.

Our mission is to enable everyone to develop, optimize and deploy AI models natively on everyone's devices.

Everything runs locally with no server support and accelerated with local GPUs on your phone and laptops. Supported platforms include:
- iPhone, iPad
- Metal GPUs and Intel/ARM MacBooks;
- AMD, Intel and NVIDIA GPUs via Vulkan on Windows and Linux;
- NVIDIA GPUs via CUDA on Windows and Linux;
- WebGPU on browsers (through companion project WebLLM).

### MPT-7B
- https://www.mosaicml.com/blog/mpt-7b
- https://huggingface.co/mosaicml/mpt-7b

MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

Introducing MPT-7B, the latest entry in our MosaicML Foundation Series. MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. MPT-7B was trained on the MosaicML platform in 9.5 days with zero human intervention at a cost of ~$200k. Starting today, you can train, finetune, and deploy your own private MPT models, either starting from one of our checkpoints or training from scratch. For inspiration, we are also releasing three finetuned models in addition to the base MPT-7B: MPT-7B-Instruct, MPT-7B-Chat, and MPT-7B-StoryWriter-65k+, the last of which uses a context length of 65k tokens!

### OpenChatKit
- https://www.together.xyz/blog/openchatkit 
- https://huggingface.co/spaces/togethercomputer/OpenChatKit
- https://github.com/togethercomputer/OpenChatKit

OpenChatKit uses a 20 billion parameter chat model trained on 43 million instructions and supports reasoning, multi-turn conversation, knowledge and generative answers.

OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned 20 billion parameter language model, a 6 billion parameter moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories. It was trained on the OIG-43M training dataset, which was a collaboration between Together, LAION, and Ontocord.ai. Much more than a model release, this is the beginning of an open source project. We are releasing a set of tools and processes for ongoing improvement with community contributions.

### Open-Assistant
- https://github.com/LAION-AI/Open-Assistant
- https://open-assistant.io/zh

Open Assistant is a project meant to give everyone access to a great chat based large language model.

We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.

### PandaLM
- https://github.com/WeOpenML/PandaLM
- https://zhuanlan.zhihu.com/p/630173415
- https://mp.weixin.qq.com/s/HE6jez3G9aEO5qLkvwtKXg

This is the official repository for PandaLM: ReProducible and Automated Language Model Assessment.

PandaLM aims to provide reproducible and automated comparisons between different large language models (LLMs). By giving PandaLM the same context, it can compare the responses of different LLMs and provide a reason for the decision, along with a reference answer. The target audience for PandaLM may be organizations that have confidential data and research labs with limited funds that seek reproducibility. These organizations may not want to disclose their data to third parties or may not be able to afford the high costs of secret data leakage using third-party APIs or hiring human annotators. With PandaLM, they can perform evaluations without compromising data security or incurring high costs, and obtain reproducible results. To demonstrate the reliability and consistency of our tool, we have created a diverse human-annotated test dataset of approximately 1,000 samples, where the contexts and the labels are all created by humans. On our test dataset, PandaLM-7B has achieved 94% ChatGPT's evaluation ability in terms of accuracy. The papers and more features are coming soon.

### PKU-Beaver æ²³ç‹¸ (Safe RLHF)
- https://github.com/PKU-Alignment/safe-rlhf
- https://mp.weixin.qq.com/s/ZpkgszXbisl5xf63EfTNjQ

åŒ—äº¬å¤§å­¦å›¢é˜Ÿå¼€æºäº†åä¸º PKU-Beaverï¼ˆæ²³ç‹¸ï¼‰é¡¹ç›®ï¼Œå…¶å¼€æºåœ°å€ä¸ºï¼šhttps://github.com/PKU-Alignment/safe-rlhfã€‚è¯¥é¡¹ç›®é¦–æ¬¡å…¬å¼€äº† RLHF æ‰€éœ€çš„æ•°æ®é›†ã€è®­ç»ƒå’ŒéªŒè¯ä»£ç ï¼Œæ˜¯ç›®å‰é¦–ä¸ªå¼€æºçš„å¯å¤ç°çš„ RLHF åŸºå‡†ã€‚åŒæ—¶ï¼Œä¸ºè§£å†³äººç±»æ ‡æ³¨äº§ç”Ÿçš„åè§å’Œæ­§è§†ç­‰ä¸å®‰å…¨å› ç´ ï¼ŒåŒ—äº¬å¤§å­¦å›¢é˜Ÿé¦–æ¬¡æå‡ºäº†å¸¦æœ‰çº¦æŸçš„ä»·å€¼å¯¹é½æŠ€æœ¯ CVAï¼ˆConstrained Value Alignmentï¼‰ã€‚è¯¥æŠ€æœ¯é€šè¿‡å¯¹æ ‡æ³¨ä¿¡æ¯è¿›è¡Œç»†ç²’åº¦åˆ’åˆ†ï¼Œå¹¶ç»“åˆå¸¦çº¦æŸçš„å®‰å…¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ˜¾è‘—é™ä½äº†æ¨¡å‹çš„åè§å’Œæ­§è§†ï¼Œæé«˜äº†æ¨¡å‹çš„å®‰å…¨æ€§ã€‚Beaverä½¿ç”¨GPT4è¿›è¡ŒEvaluationï¼Œç»“æœè¡¨æ˜ï¼Œåœ¨åŸæœ‰æ€§èƒ½ä¿æŒä¸å˜çš„æƒ…å†µä¸‹ï¼ŒBeaverå›å¤çš„å®‰å…¨æ€§å¤§å¹…åº¦æå‡ã€‚

### PaLM + RLHF (Pytorch)
- https://github.com/lucidrains/PaLM-rlhf-pytorch

Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Maybe I'll add retrieval functionality too, Ã  la RETRO

### * ã€Guanaco & QloRAã€‘
- https://mp.weixin.qq.com/s/SGJQHsEJTNB6hiVqdc87sg
- https://arxiv.org/abs/2305.14314
- https://github.com/artidoro/qlora

We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training.


### RL4LMs
- https://github.com/allenai/RL4LMs
- https://rl4lms.apps.allenai.org/

A modular RL library to fine-tune language models to human preferences

We provide easily customizable building blocks for training language models including implementations of on-policy algorithms, reward functions, metrics, datasets and LM based actor-critic policies

### Reinforcement Learning with Language Model
- https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å¼€æºé¡¹ç›® trl æ­å»ºä¸€ä¸ªé€šè¿‡å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼ˆPPOï¼‰æ¥æ›´æ–°è¯­è¨€æ¨¡å‹ï¼ˆGPT-2ï¼‰çš„å‡ ä¸ªç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
- åŸºäºä¸­æ–‡æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹çš„æ­£å‘è¯„è®ºç”Ÿæˆæœºå™¨äººï¼ˆNo Human Rewardï¼‰
- åŸºäºäººå·¥æ‰“åˆ†çš„æ­£å‘è¯„è®ºç”Ÿæˆæœºå™¨äººï¼ˆWith Human Rewardï¼‰
- åŸºäºæ’åºåºåˆ—ï¼ˆRank Listï¼‰è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆReward Modelï¼‰
- æ’åºåºåˆ—ï¼ˆRank Listï¼‰æ ‡æ³¨å¹³å°

### StableLM
- https://zhuanlan.zhihu.com/p/623542189
- https://github.com/Stability-AI/StableLM

StableLM: Stability AI Language Models

This repository contains Stability AI's ongoing development of the StableLM series of language models and will be continuously updated with new checkpoints. The following provides an overview of all currently available models. More coming soon.

### StableVicuna
- https://github.com/Stability-AI/StableLM

StableVicunaåŸºäºå°ç¾Šé©¼Vicuna-13Bçš„è¿›ä¸€æ­¥æŒ‡ä»¤å¾®è°ƒå’ŒRLHFè®­ç»ƒçš„ç‰ˆæœ¬ã€‚Vicuna-13Bæ˜¯LLaMA-13Bçš„ä¸€ä¸ªæŒ‡ä»¤å¾®è°ƒæ¨¡å‹ã€‚

### Stanford Alpaca
- https://crfm.stanford.edu/2023/03/13/alpaca.html
- https://alpaca-ai.ngrok.io/
- https://github.com/tatsu-lab/stanford_alpaca

Alpaca: A Strong, Replicable Instruction-Following ModelAl

We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAIâ€™s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$).

### Transformer Reinforcement Learning
- https://github.com/lvwerra/trl

With trl you can train transformer language models with Proximal Policy Optimization (PPO). The library is built on top of the transformers library by ğŸ¤— Hugging Face. Therefore, pre-trained language models can be directly loaded via transformers. At this point most of decoder architectures and encoder-decoder architectures are supported.

### Transformer Reinforcement Learning X
- https://github.com/CarperAI/trlx

trlX is a distributed training framework designed from the ground up to focus on fine-tuning large language models with reinforcement learning using either a provided reward function or a reward-labeled dataset.

Training support for ğŸ¤— Hugging Face models is provided by Accelerate-backed trainers, allowing users to fine-tune causal and T5-based language models of up to 20B parameters, such as facebook/opt-6.7b, EleutherAI/gpt-neox-20b, and google/flan-t5-xxl. For models beyond 20B parameters, trlX provides NVIDIA NeMo-backed trainers that leverage efficient parallelism techniques to scale effectively.

### Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality
- https://chat.lmsys.org/
- https://vicuna.lmsys.org/
- https://github.com/lm-sys/FastChat

An open platform for training, serving, and evaluating large language model based chatbots.

### Wombat
- https://mp.weixin.qq.com/s/xoPKmOzjlNZ2qGdcKeGARw
- https://mp.weixin.qq.com/s/UI-ij5o43ct1efYoNVdQDg
- https://arxiv.org/abs/2304.05302v1
- https://github.com/GanjinZero/RRHF

This is the repository for RRHF (Rank Response to align Human Feedback) and open-sourced language models Wombat. RRHF helps align large language models with human perference easier.

Reinforcement Learning from Human Feedback (RLHF) enables the alignment of large language models with human preference, improving the quality of interactions between humans and language models. Recent practice of RLHF uses PPO to enable the large language model optimization of such alignment. However, implementing PPO is non-trivial (where the training procedure requires interactive between policy, behavior policy, reward, value model) and it is also tedious to tuning many hyper-parameters. Our motivation is to simplify the alignment between language models with human preference, and our proposed paradigm RRHF (Rank Response from Human Feedback) can achieve such alignment as easily as conventional fine-tuning. It is simpler than PPO from the aspects of coding, model counts, and hyperparameters.

## 2 ä¸­æ–‡å¼€æºæ¨¡å‹ï¼ˆChinese Open Source Language Modelsï¼‰

### åä½—
- https://zhuanlan.zhihu.com/p/626536996
- https://github.com/scir-hi/huatuo-llama-med-chinese

åä½—: åŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„LLaMaæŒ‡ä»¤å¾®è°ƒæ¨¡å‹

åœ¨ç”Ÿç‰©åŒ»å­¦é¢†åŸŸï¼ŒLLMæ¨¡å‹ï¼ˆå¦‚LLaMaï¼ŒChatGLMï¼‰å› ä¸ºç¼ºä¹ä¸€å®šçš„åŒ»å­¦ä¸“ä¸šçŸ¥è¯†è¯­æ–™è€Œè¡¨ç°ä¸ä½³ã€‚è¯¥é¡¹ç›®é€šè¿‡åŒ»å­¦çŸ¥è¯†å›¾è°±å’ŒGPT3.5APIæ„å»ºäº†ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶å¯¹LLaMaæ¨¡å‹è¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒå¾—åˆ°äº†ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦é¢†åŸŸçš„æ™ºèƒ½é—®è¯Šæ¨¡å‹HuaTuoï¼Œç›¸æ¯”äºæœªç»è¿‡åŒ»å­¦æ•°æ®æŒ‡ä»¤å¾®è°ƒçš„åŸLLaMaè€Œè¨€ï¼ŒHuaTuoæ¨¡å‹åœ¨æ™ºèƒ½é—®è¯Šå±‚é¢è¡¨ç°å‡ºè‰²ï¼Œå¯ç”Ÿæˆä¸€äº›æ›´ä¸ºå¯é çš„åŒ»å­¦çŸ¥è¯†å›ç­”ï¼›ä¸æ­¤åŒæ—¶ï¼ŒåŸºäºç›¸åŒåŒ»å­¦æ•°æ®ï¼Œè¯¥é¡¹ç›®è¿˜è®­ç»ƒäº†åŒ»ç–—ç‰ˆæœ¬çš„ChatGLMæ¨¡å‹: ChatGLM-6B-Medï¼Œ

é™¤äº†åä½—æ¨¡å‹ï¼Œè¯¥å›¢é˜Ÿè¿˜å³å°†å‘å¸ƒæ‰é¹Šæ¨¡å‹PienChueh(åŒä¸ºåŸºäºåŒ»å­¦æ•°æ®è®­ç»ƒçš„å¤§æ¨¡å‹)ï¼Œæ¬¢è¿å¤§å®¶å±Šæ—¶ä½¿ç”¨ä½“éªŒã€‚

### ä¸­æ–‡Alpacaæ¨¡å‹Luotuo
- https://sota.jiqizhixin.com/project/luotuo
- https://github.com/LC1332/Luotuo-Chinese-LLM

Alpaca æ˜¯æ–¯å¦ç¦å›¢é˜ŸåŸºäº LLaMA 7B åœ¨ 52k æŒ‡ä»¤ä¸Šå¾®è°ƒå¾—åˆ°çš„æ¨¡å‹ï¼Œèƒ½å‡ºè‰²é€‚åº”å¤šç§è‡ªç„¶è¯­è¨€åº”ç”¨åœºæ™¯ã€‚è¿‘æ—¥æ¥è‡ªå•†æ±¤ç§‘æŠ€å’Œåä¸­ç§‘æŠ€å¤§å­¦å¼€æºä¸­æ–‡è¯­è¨€æ¨¡å‹ Luotuoï¼ŒåŸºäº ChatGPT API ç¿»è¯‘ Alpaca å¾®è°ƒæŒ‡ä»¤æ•°æ®ï¼Œå¹¶ä½¿ç”¨ lora è¿›è¡Œå¾®è°ƒå¾—åˆ°ã€‚ç›®å‰è¯¥é¡¹ç›®å·²å…¬å¼€è®­ç»ƒçš„è¯­æ–™å’Œæ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆä¸¤ä¸ªå‹å·ï¼‰ï¼Œä¾›å¼€å‘è€…å¯ä½¿ç”¨è‡ªå·±å„ç§å¤§å°çš„è¯­æ–™ï¼Œè®­ç»ƒè‡ªå·±çš„è¯­è¨€æ¨¡å‹ï¼Œå¹¶é€‚ç”¨åˆ°å¯¹åº”çš„å‚ç›´é¢†åŸŸã€‚

### ä¸­æ–‡LLaMA&Alpacaå¤§æ¨¡å‹
- https://github.com/ymcui/Chinese-LLaMA-Alpaca

ä»¥ChatGPTã€GPT-4ç­‰ä¸ºä»£è¡¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰æ€èµ·äº†æ–°ä¸€è½®è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç ”ç©¶æµªæ½®ï¼Œå±•ç°å‡ºäº†ç±»é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„èƒ½åŠ›ï¼Œå—åˆ°ä¸šç•Œå¹¿æ³›å…³æ³¨ã€‚ç„¶è€Œï¼Œç”±äºå¤§è¯­è¨€æ¨¡å‹çš„è®­ç»ƒå’Œéƒ¨ç½²éƒ½æä¸ºæ˜‚è´µï¼Œä¸ºæ„å»ºé€æ˜ä¸”å¼€æ”¾çš„å­¦æœ¯ç ”ç©¶é€ æˆäº†ä¸€å®šçš„é˜»ç¢ã€‚

ä¸ºäº†ä¿ƒè¿›å¤§æ¨¡å‹åœ¨ä¸­æ–‡NLPç¤¾åŒºçš„å¼€æ”¾ç ”ç©¶ï¼Œæœ¬é¡¹ç›®å¼€æºäº†ä¸­æ–‡LLaMAæ¨¡å‹å’Œç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œåœ¨ä¸­æ–‡LLaMAçš„åŸºç¡€ä¸Šï¼Œæœ¬é¡¹ç›®ä½¿ç”¨äº†ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡ŒæŒ‡ä»¤ç²¾è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚

### ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹Firefly
- https://mp.weixin.qq.com/s/tyH9Ifcvw4DKqoIoYjT6Kg
- https://github.com/yangjianxin1/Firefly

Fireflyï¼ˆæµè¤ï¼‰ æ˜¯ä¸€ä¸ªå¼€æºçš„ä¸­æ–‡å¯¹è¯å¼å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šè¿›è¡Œè°ƒä¼˜ã€‚åŒæ—¶ä½¿ç”¨äº†è¯è¡¨è£å‰ªã€ZeROã€å¼ é‡å¹¶è¡Œç­‰æŠ€æœ¯ï¼Œæœ‰æ•ˆé™ä½æ˜¾å­˜æ¶ˆè€—å’Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚ åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ›´å°çš„æ¨¡å‹å‚æ•°é‡ï¼Œä»¥åŠæ›´å°‘çš„è®¡ç®—èµ„æºã€‚

æˆ‘ä»¬æ„é€ äº†è®¸å¤šä¸ä¸­åæ–‡åŒ–ç›¸å…³çš„æ•°æ®ï¼Œä»¥æå‡æ¨¡å‹è¿™æ–¹é¢çš„è¡¨ç°ï¼Œå¦‚å¯¹è”ã€ä½œè¯—ã€æ–‡è¨€æ–‡ç¿»è¯‘ã€æ•£æ–‡ã€é‡‘åº¸å°è¯´ç­‰ã€‚

### å‡¤å‡°
- https://mp.weixin.qq.com/s/beAAh_MdqssV8bEKsccElg
- https://github.com/FreedomIntelligence/LLMZoo

LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.

### å¤æ—¦MOSS
- https://github.com/OpenLMLab/MOSS
- https://mp.weixin.qq.com/s/LjToZVWjQ-ot5KJFCFtA3g

MOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œmoss-moonç³»åˆ—æ¨¡å‹å…·æœ‰160äº¿å‚æ•°ï¼Œåœ¨FP16ç²¾åº¦ä¸‹å¯åœ¨å•å¼ A100/A800æˆ–ä¸¤å¼ 3090æ˜¾å¡è¿è¡Œï¼Œåœ¨INT4/8ç²¾åº¦ä¸‹å¯åœ¨å•å¼ 3090æ˜¾å¡è¿è¡Œã€‚MOSSåŸºåº§è¯­è¨€æ¨¡å‹åœ¨çº¦ä¸ƒåƒäº¿ä¸­è‹±æ–‡ä»¥åŠä»£ç å•è¯ä¸Šé¢„è®­ç»ƒå¾—åˆ°ï¼Œåç»­ç»è¿‡å¯¹è¯æŒ‡ä»¤å¾®è°ƒã€æ’ä»¶å¢å¼ºå­¦ä¹ å’Œäººç±»åå¥½è®­ç»ƒå…·å¤‡å¤šè½®å¯¹è¯èƒ½åŠ›åŠä½¿ç”¨å¤šç§æ’ä»¶çš„èƒ½åŠ›ã€‚

### BELLE: Bloom-Enhanced Large Language model Engine
- https://huggingface.co/BelleGroup
- https://github.com/LianjiaTech/BELLE

æœ¬é¡¹ç›®åŸºäº Stanford Alpaca ï¼ŒStanford Alpaca çš„ç›®æ ‡æ˜¯æ„å»ºå’Œå¼€æºä¸€ä¸ªåŸºäºLLaMAçš„æ¨¡å‹ã€‚ Stanford Alpaca çš„ç§å­ä»»åŠ¡éƒ½æ˜¯è‹±è¯­ï¼Œæ”¶é›†çš„æ•°æ®ä¹Ÿéƒ½æ˜¯è‹±æ–‡ï¼Œå› æ­¤è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æœªå¯¹ä¸­æ–‡ä¼˜åŒ–ã€‚


æœ¬é¡¹ç›®ç›®æ ‡æ˜¯ä¿ƒè¿›ä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹å¼€æºç¤¾åŒºçš„å‘å±•ã€‚æœ¬é¡¹ç›®é’ˆå¯¹ä¸­æ–‡åšäº†ä¼˜åŒ–ï¼Œæ¨¡å‹è°ƒä¼˜ä»…ä½¿ç”¨ç”±ChatGPTç”Ÿäº§çš„æ•°æ®ï¼ˆä¸åŒ…å«ä»»ä½•å…¶ä»–æ•°æ®ï¼‰ã€‚

### Bloom
- https://huggingface.co/blog/bloom
- https://huggingface.co/bigscience/bloom

BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.

### BiLLa: A Bilingual LLaMA with Enhanced Reasoning Ability
- https://zhuanlan.zhihu.com/p/628688680
- https://github.com/Neutralzz/BiLLa

BiLLaæ˜¯å¼€æºçš„æ¨ç†èƒ½åŠ›å¢å¼ºçš„ä¸­è‹±åŒè¯­LLaMAæ¨¡å‹ã€‚æ¨¡å‹çš„ä¸»è¦ç‰¹æ€§æœ‰ï¼š
- è¾ƒå¤§æå‡LLaMAçš„ä¸­æ–‡ç†è§£èƒ½åŠ›ï¼Œå¹¶å°½å¯èƒ½å‡å°‘å¯¹åŸå§‹LLaMAè‹±æ–‡èƒ½åŠ›çš„æŸä¼¤ï¼›
- è®­ç»ƒè¿‡ç¨‹å¢åŠ è¾ƒå¤šçš„ä»»åŠ¡å‹æ•°æ®ï¼Œåˆ©ç”¨ChatGPTç”Ÿæˆè§£æï¼Œå¼ºåŒ–æ¨¡å‹ç†è§£ä»»åŠ¡æ±‚è§£é€»è¾‘ï¼›
- å…¨é‡å‚æ•°æ›´æ–°ï¼Œè¿½æ±‚æ›´å¥½çš„ç”Ÿæˆæ•ˆæœã€‚

### BLOOMChat176B
- https://mp.weixin.qq.com/s/cY6ORD8CUyXRL0l20EjwqQ
- https://sambanova.ai/blog/introducing-bloomchat-176b-the-multilingual-chat-based-llm/
- https://huggingface.co/spaces/sambanovasystems/BLOOMChat
- https://github.com/sambanova/bloomchat

å¼€æºå¯¹è¯æ¨¡å‹ä¸€ç›´è·Ÿé—­æºæ¨¡å‹åœ¨å¤šè¯­è¨€èƒ½åŠ›ä¸Šå­˜åœ¨å·®è·ã€‚SambaNova å’Œæ–¯å¦ç¦ Together Computer å¼€æºå¯å•†ç”¨çš„å¤šè¯­è¨€èŠå¤©æ¨¡å‹ BLOOMChat 176Bï¼Œæ”¯æŒä¸­æ–‡ã€‚BLOOMChat åœ¨SambaNova è‡ªç ”èŠ¯ç‰‡ RDU ä¸Šå®Œæˆè®­ç»ƒï¼Œå€ŸåŠ© SambaNova çš„ç‹¬ç‰¹å¯é‡æ„æ•°æ®æµæ¶æ„ï¼Œåˆ©ç”¨ BLOOM å¼€æºæ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œé€šè¿‡åœ¨ OpenChatKitã€Dolly 2.0 å’Œ OASST1 çš„ OIG ä¸Šè¿›è¡Œå¾®è°ƒã€‚åœ¨åŸºäºå…­ç§è¯­è¨€çš„æ—©æœŸåŒç›²æµ‹è¯•ä¸­ï¼ŒBLOOMChat åœ¨ 66%çš„æµ‹è¯„æ•°æ®ä¸Šäº§ç”Ÿçš„å¯¹è¯è¡¨ç°ä¼˜äºè¿‘æœŸçš„å¼€æºå¯¹è¯æ¨¡å‹ã€‚åŒæ—¶åœ¨ä¸ GPT4 çš„åŸºäºå…­ç§è¯­è¨€çš„äººå·¥æµ‹è¯„å¯¹æ¯”ä¸­ï¼ŒBLOOMChat å¾—åˆ° 45%å¯¹ 55%çš„èƒœç‡ï¼Œå¤§å¤§ç¼©å°å¼€æºå’Œé—­æºæ¨¡å‹çš„å¤šè¯­è¨€å¯¹è¯èƒ½åŠ›å·®è·ã€‚å½“å‰ BLOOMChat å¼€æºæ¨¡å‹æ–‡ä»¶ï¼Œæ”¯æŒåœ¨ huggingface åœ¨çº¿æ¨ç†è¯•ç”¨ã€‚

### ChatYuan
- https://github.com/clue-ai/ChatYuan
- https://modelscope.cn/models/ClueAI/ChatYuan-large

å…ƒè¯­åŠŸèƒ½å‹å¯¹è¯å¤§æ¨¡å‹, è¿™ä¸ªæ¨¡å‹å¯ä»¥ç”¨äºé—®ç­”ã€ç»“åˆä¸Šä¸‹æ–‡åšå¯¹è¯ã€åšå„ç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬åˆ›æ„æ€§å†™ä½œï¼Œä¹Ÿèƒ½å›ç­”ä¸€äº›åƒæ³•å¾‹ã€æ–°å† ç­‰é¢†åŸŸé—®é¢˜ã€‚å®ƒåŸºäºPromptCLUE-largeç»“åˆæ•°äº¿æ¡åŠŸèƒ½å¯¹è¯å¤šè½®å¯¹è¯æ•°æ®è¿›ä¸€æ­¥è®­ç»ƒå¾—åˆ°ã€‚

PromptCLUE-largeåœ¨1000äº¿tokenä¸­æ–‡è¯­æ–™ä¸Šé¢„è®­ç»ƒï¼Œç´¯è®¡å­¦ä¹ 1.5ä¸‡äº¿ä¸­æ–‡tokenï¼Œå¹¶ä¸”åœ¨æ•°ç™¾ç§ä»»åŠ¡ä¸Šè¿›è¡ŒPromptä»»åŠ¡å¼è®­ç»ƒã€‚é’ˆå¯¹ç†è§£ç±»ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æŠ½å–ç­‰ï¼Œå¯ä»¥è‡ªå®šä¹‰æ ‡ç­¾ä½“ç³»ï¼›é’ˆå¯¹å¤šç§ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è¿›è¡Œé‡‡æ ·è‡ªç”±ç”Ÿæˆã€‚

### ChatGLM-6B
- https://github.com/THUDM/ChatGLM-6B
- https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning

ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº General Language Model (GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¶ˆè´¹çº§çš„æ˜¾å¡ä¸Šè¿›è¡Œæœ¬åœ°éƒ¨ç½²ï¼ˆINT4 é‡åŒ–çº§åˆ«ä¸‹æœ€ä½åªéœ€ 6GB æ˜¾å­˜ï¼‰ã€‚ ChatGLM-6B ä½¿ç”¨äº†å’Œ ChatGPT ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œé’ˆå¯¹ä¸­æ–‡é—®ç­”å’Œå¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç»è¿‡çº¦ 1T æ ‡è¯†ç¬¦çš„ä¸­è‹±åŒè¯­è®­ç»ƒï¼Œè¾…ä»¥ç›‘ç£å¾®è°ƒã€åé¦ˆè‡ªåŠ©ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯çš„åŠ æŒï¼Œ62 äº¿å‚æ•°çš„ ChatGLM-6B å·²ç»èƒ½ç”Ÿæˆç›¸å½“ç¬¦åˆäººç±»åå¥½çš„å›ç­”ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒæˆ‘ä»¬çš„åšå®¢ã€‚

### Chinese-Transformer-XL
- https://github.com/THUDM/Chinese-Transformer-XL

æœ¬é¡¹ç›®æä¾›äº†æ™ºæºç ”ç©¶é™¢"æ–‡æ±‡" é¢„è®­ç»ƒæ¨¡å‹Chinese-Transformer-XLçš„é¢„è®­ç»ƒå’Œæ–‡æœ¬ç”Ÿæˆä»£ç ã€‚

### ChatMed-TCM & ChatMed-Consult
- https://github.com/michael-wzhu/ChatMed

ğŸš€ ChatMed-Consult : åŸºäºä¸­æ–‡åŒ»ç–—åœ¨çº¿é—®è¯Šæ•°æ®é›†ChatMed_Consult_Datasetçš„50w+åœ¨çº¿é—®è¯Š+ChatGPTå›å¤ä½œä¸ºè®­ç»ƒé›†ã€‚æ¨¡å‹ä¸»å¹²ä¸ºLlaMA-7b,èåˆäº†Chinese-LlaMA-Alpacaçš„LoRAæƒé‡ä¸ä¸­æ–‡æ‰©å±•è¯è¡¨ï¼Œç„¶åå†è¿›è¡ŒåŸºäºLoRAçš„å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚æˆ‘ä»¬å°†å…¨éƒ¨ä»£ç éƒ½è¿›è¡Œäº†å…¬å¼€ã€‚æˆ‘ä»¬ä¹Ÿå°†éƒ¨ç½²ä¸€ä¸ªåœ¨çº¿Gradio demo, æ•¬è¯·å…³æ³¨ã€‚

â³ ChatMed-TCM : å¤§æ¨¡å‹èµ‹èƒ½ä¸­åŒ»è¯ä¼ æ‰¿ã€‚è¿™ä¸€æ¨¡å‹çš„è®­ç»ƒæ•°æ®ä¸ºä¸­åŒ»è¯æŒ‡ä»¤æ•°æ®é›†ChatMed_TCM_Datasetã€‚ä»¥æˆ‘ä»¬å¼€æºçš„ä¸­åŒ»è¯çŸ¥è¯†å›¾è°±ä¸ºåŸºç¡€ï¼Œé‡‡ç”¨ä»¥å®ä½“ä¸ºä¸­å¿ƒçš„è‡ªæŒ‡ä»¤æ–¹æ³•(entity-centric self-instruct)ï¼Œè°ƒç”¨ChatGPTå¾—åˆ°2.6w+çš„å›´ç»•ä¸­åŒ»è¯çš„æŒ‡ä»¤æ•°æ®ã€‚ChatMed-TCMæ¨¡å‹ä¹Ÿæ˜¯ä»¥LlaMAä¸ºåº•åº§ï¼Œé‡‡ç”¨LoRAå¾®è°ƒå¾—åˆ°ã€‚

### ChatGLM-Med
- https://github.com/SCIR-HI/Med-ChatGLM

åŸºäºä¸­æ–‡åŒ»å­¦çŸ¥è¯†çš„ChatGLMæ¨¡å‹å¾®è°ƒï¼Œæœ¬é¡¹ç›®å¼€æºäº†ç»è¿‡ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤ç²¾è°ƒ/æŒ‡ä»¤å¾®è°ƒ(Instruct-tuning) çš„ChatGLM-6Bæ¨¡å‹ã€‚æˆ‘ä»¬é€šè¿‡åŒ»å­¦çŸ¥è¯†å›¾è°±å’ŒGPT3.5 APIæ„å»ºäº†ä¸­æ–‡åŒ»å­¦æŒ‡ä»¤æ•°æ®é›†ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šå¯¹ChatGLM-6Bè¿›è¡Œäº†æŒ‡ä»¤å¾®è°ƒï¼Œæé«˜äº†ChatGLMåœ¨åŒ»ç–—é¢†åŸŸçš„é—®ç­”æ•ˆæœã€‚

### DoctorGLM
- https://github.com/xionghonglin/DoctorGLM

DoctorGLMï¼ŒåŸºäº ChatGLM-6Bçš„ä¸­æ–‡é—®è¯Šæ¨¡å‹ã€‚

### EVA: å¤§è§„æ¨¡ä¸­æ–‡å¼€æ”¾åŸŸå¯¹è¯ç³»ç»Ÿ
- https://github.com/thu-coai/EVA

EVA æ˜¯ç›®å‰æœ€å¤§çš„å¼€æºä¸­æ–‡é¢„è®­ç»ƒå¯¹è¯æ¨¡å‹ï¼Œæ‹¥æœ‰28äº¿å‚æ•°ï¼Œä¸»è¦æ“…é•¿å¼€æ”¾åŸŸé—²èŠï¼Œç›®å‰æœ‰ 1.0 å’Œ 2.0 ä¸¤ä¸ªç‰ˆæœ¬ã€‚å…¶ä¸­ï¼Œ1.0ç‰ˆæœ¬åœ¨ WudaoCorpus-Dialog ä¸Šè®­ç»ƒè€Œæˆï¼Œ2.0 ç‰ˆæœ¬åœ¨ä» WudaoCorpus-Dialog ä¸­æ¸…æ´—å‡ºçš„æ›´é«˜è´¨é‡çš„å¯¹è¯æ•°æ®ä¸Šè®­ç»ƒè€Œæˆï¼Œæ¨¡å‹æ€§èƒ½ä¹Ÿæ˜æ˜¾å¥½äº EVA1.0ã€‚

### GPT2 for Multiple Language
- https://github.com/imcaspar/gpt2-ml

- ç®€åŒ–æ•´ç† GPT2 è®­ç»ƒä»£ç ï¼ˆbased on Grover, supporting TPUsï¼‰
- ç§»æ¤ bert tokenizerï¼Œæ·»åŠ å¤šè¯­è¨€æ”¯æŒ
- 15äº¿å‚æ•° GPT2 ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹( 15G è¯­æ–™ï¼Œè®­ç»ƒ 10w æ­¥ )
- å¼€ç®±å³ç”¨çš„æ¨¡å‹ç”Ÿæˆæ•ˆæœ demo #
- 15äº¿å‚æ•° GPT2 ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹( 30G è¯­æ–™ï¼Œè®­ç»ƒ 22w æ­¥ )

### LawGPT_zh ä¸­æ–‡æ³•å¾‹å¤§æ¨¡å‹ï¼ˆç¬è±¸ï¼‰
- https://mp.weixin.qq.com/s/Pk4NdFQq5G6iZ3QmcyyFUg
- https://github.com/LiuHC0428/LAW-GPT

æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯ä¸ºè®©æ‰€æœ‰äººåœ¨é‡åˆ°æ³•å¾‹é—®é¢˜æ—¶èƒ½ç¬¬ä¸€æ—¶é—´è·å¾—ä¸“ä¸šå¯é çš„å›ç­”ã€‚å› ä¸ºä¸“ä¸šçš„å¾‹å¸ˆæœåŠ¡åªæœ‰çœŸæ­£è§¦æ‰‹å¯åŠï¼Œæ‰ä¼šè®©äººä»¬ä¹ æƒ¯è¿ç”¨ï¼Œä¸€å¦‚äºŒåå¹´å‰çš„æœç´¢å¼•æ“ï¼Œåå¹´å‰çš„å¿«é€’ä¸šåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›è®©æ³•å¾‹èµ°è¿›æ—¥å¸¸ç”Ÿæ´»ï¼Œä¸ºæ„å»ºæ³•æ²»ç¤¾ä¼šè´¡çŒ®æˆ‘ä»¬çš„åŠ›é‡ã€‚é¡¹ç›®æµ·æŠ¥ç”±Midjourneyç”Ÿæˆã€‚

æœ¬é¡¹ç›®å¼€æºçš„ä¸­æ–‡æ³•å¾‹é€šç”¨æ¨¡å‹ç”±ChatGLM-6B LoRA 16-bitæŒ‡ä»¤å¾®è°ƒå¾—åˆ°ã€‚æ•°æ®é›†åŒ…æ‹¬ç°æœ‰çš„æ³•å¾‹é—®ç­”æ•°æ®é›†å’ŒåŸºäºæ³•æ¡å’ŒçœŸå®æ¡ˆä¾‹æŒ‡å¯¼çš„self-Instructæ„å»ºçš„é«˜è´¨é‡æ³•å¾‹æ–‡æœ¬é—®ç­”ï¼Œæé«˜äº†é€šç”¨è¯­è¨€å¤§æ¨¡å‹åœ¨æ³•å¾‹é¢†åŸŸçš„è¡¨ç°ï¼Œæé«˜äº†æ¨¡å‹å›ç­”çš„å¯é æ€§å’Œä¸“ä¸šç¨‹åº¦ã€‚

### Linlyä¼¶è”è¯´
- https://github.com/CVI-SZU/Linly
- https://mp.weixin.qq.com/s/zSxsArP1pxYNubNDZua7iA

â€œä¼¶è”è¯´â€æ¨¡å‹å…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š1. åœ¨32*A100 GPUä¸Šè®­ç»ƒäº†ä¸åŒé‡çº§å’ŒåŠŸèƒ½çš„ä¸­æ–‡æ¨¡å‹ï¼Œå¯¹æ¨¡å‹å……åˆ†è®­ç»ƒå¹¶æä¾›å¼ºå¤§çš„baselineã€‚æ®æˆ‘ä»¬æ‰€çŸ¥33Bçš„Linly-Chinese-LLAMAæ˜¯ç›®å‰æœ€å¤§çš„ä¸­æ–‡LLaMAæ¨¡å‹ã€‚2. å…¬å¼€æ‰€æœ‰è®­ç»ƒæ•°æ®ã€ä»£ç ã€å‚æ•°ç»†èŠ‚ä»¥åŠå®éªŒç»“æœï¼Œç¡®ä¿é¡¹ç›®çš„å¯å¤ç°æ€§ï¼Œç”¨æˆ·å¯ä»¥é€‰æ‹©åˆé€‚çš„èµ„æºç›´æ¥ç”¨äºè‡ªå·±çš„æµç¨‹ä¸­ã€‚3. é¡¹ç›®å…·æœ‰é«˜å…¼å®¹æ€§å’Œæ˜“ç”¨æ€§ï¼Œæä¾›å¯ç”¨äºCUDAå’ŒCPUçš„é‡åŒ–æ¨ç†æ¡†æ¶ï¼Œå¹¶æ”¯æŒHuggingfaceæ ¼å¼ã€‚

ç›®å‰å…¬å¼€å¯ç”¨çš„æ¨¡å‹æœ‰ï¼š

Linly-Chinese-LLaMAï¼šä¸­æ–‡åŸºç¡€æ¨¡å‹ï¼ŒåŸºäºLLaMAåœ¨é«˜è´¨é‡ä¸­æ–‡è¯­æ–™ä¸Šå¢é‡è®­ç»ƒå¼ºåŒ–ä¸­æ–‡è¯­è¨€èƒ½åŠ›ï¼Œç°å·²å¼€æ”¾ 7Bã€13B å’Œ 33B é‡çº§ï¼Œ65Bæ­£åœ¨è®­ç»ƒä¸­ã€‚

Linly-ChatFlowï¼šä¸­æ–‡å¯¹è¯æ¨¡å‹ï¼Œåœ¨400ä¸‡æŒ‡ä»¤æ•°æ®é›†åˆä¸Šå¯¹ä¸­æ–‡åŸºç¡€æ¨¡å‹æŒ‡ä»¤ç²¾è°ƒï¼Œç°å·²å¼€æ”¾7Bã€13Bå¯¹è¯æ¨¡å‹ã€‚

Linly-ChatFlow-int4 ï¼šChatFlow 4-bité‡åŒ–ç‰ˆæœ¬ï¼Œç”¨äºåœ¨CPUä¸Šéƒ¨ç½²æ¨¡å‹æ¨ç†ã€‚

è¿›è¡Œä¸­çš„é¡¹ç›®ï¼š
Linly-Chinese-BLOOMï¼šåŸºäºBLOOMä¸­æ–‡å¢é‡è®­ç»ƒçš„ä¸­æ–‡åŸºç¡€æ¨¡å‹ï¼ŒåŒ…å«7Bå’Œ175Bæ¨¡å‹é‡çº§ï¼Œå¯ç”¨äºå•†ä¸šåœºæ™¯ã€‚

### MedicalGPT-zh
- github.com/MediaBrain-SJTU/MedicalGPT-zh

è¯¥å¼€æºäº†åŸºäºChatGLM-6B LoRA 16-bitæŒ‡ä»¤å¾®è°ƒçš„ä¸­æ–‡åŒ»ç–—é€šç”¨æ¨¡å‹ã€‚åŸºäºå…±è®¡28ç§‘å®¤çš„ä¸­æ–‡åŒ»ç–—å…±è¯†ä¸ä¸´åºŠæŒ‡å—æ–‡æœ¬ï¼Œæˆ‘ä»¬ç”ŸæˆåŒ»ç–—çŸ¥è¯†è¦†ç›–é¢æ›´å…¨ï¼Œå›ç­”å†…å®¹æ›´åŠ ç²¾å‡†çš„é«˜è´¨é‡æŒ‡ä»¤æ•°æ®é›†ã€‚

### PromptCLUE
- https://github.com/clue-ai/PromptCLUE

PromptCLUEï¼šå¤§è§„æ¨¡å¤šä»»åŠ¡Prompté¢„è®­ç»ƒä¸­æ–‡å¼€æºæ¨¡å‹ã€‚

ä¸­æ–‡ä¸Šçš„ä¸‰å¤§ç»Ÿä¸€ï¼šç»Ÿä¸€æ¨¡å‹æ¡†æ¶ï¼Œç»Ÿä¸€ä»»åŠ¡å½¢å¼ï¼Œç»Ÿä¸€åº”ç”¨æ–¹å¼ã€‚

æ”¯æŒå‡ åä¸ªä¸åŒç±»å‹çš„ä»»åŠ¡ï¼Œå…·æœ‰è¾ƒå¥½çš„é›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›å’Œå°‘æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚é’ˆå¯¹ç†è§£ç±»ä»»åŠ¡ï¼Œå¦‚åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æŠ½å–ç­‰ï¼Œå¯ä»¥è‡ªå®šä¹‰æ ‡ç­¾ä½“ç³»ï¼›é’ˆå¯¹ç”Ÿæˆä»»åŠ¡ï¼Œå¯ä»¥è¿›è¡Œé‡‡æ ·è‡ªç”±ç”Ÿæˆã€‚

åƒäº¿ä¸­æ–‡tokenä¸Šå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œç´¯è®¡å­¦ä¹ 1.5ä¸‡äº¿ä¸­æ–‡tokenï¼Œäº¿çº§ä¸­æ–‡ä»»åŠ¡æ•°æ®ä¸Šå®Œæˆè®­ç»ƒï¼Œè®­ç»ƒä»»åŠ¡è¶…è¿‡150+ã€‚æ¯”baseç‰ˆå¹³å‡ä»»åŠ¡æå‡7ä¸ªç‚¹+ï¼›å…·æœ‰æ›´å¥½çš„ç†è§£ã€ç”Ÿæˆå’ŒæŠ½å–èƒ½åŠ›ï¼Œå¹¶ä¸”æ”¯æŒæ–‡æœ¬æ”¹å†™ã€çº é”™ã€çŸ¥è¯†å›¾è°±é—®ç­”ã€‚

### SkyText-Chinese-GPT3
- https://github.com/SkyWorkAIGC/SkyText-Chinese-GPT3

SkyTextæ˜¯ç”±å¥‡ç‚¹æ™ºæºå‘å¸ƒçš„ä¸­æ–‡GPT3é¢„è®­ç»ƒå¤§æ¨¡å‹ï¼Œå¯ä»¥è¿›è¡ŒèŠå¤©ã€é—®ç­”ã€ä¸­è‹±äº’è¯‘ç­‰ä¸åŒçš„ä»»åŠ¡ã€‚ åº”ç”¨è¿™ä¸ªæ¨¡å‹ï¼Œé™¤äº†å¯ä»¥å®ç°åŸºæœ¬çš„èŠå¤©ã€å¯¹è¯ã€ä½ é—®æˆ‘ç­”å¤–ï¼Œè¿˜èƒ½æ”¯æŒä¸­è‹±æ–‡äº’è¯‘ã€å†…å®¹ç»­å†™ã€å¯¹å¯¹è”ã€å†™å¤è¯—ã€ç”Ÿæˆèœè°±ã€ç¬¬ä¸‰äººç§°è½¬è¿°ã€åˆ›å»ºé‡‡è®¿é—®é¢˜ç­‰å¤šç§åŠŸèƒ½ã€‚

## 3 å…¶ä»–å°ä¼™ä¼´çš„èµ„æ–™
### æ€»ç»“å¼€æºå¯ç”¨çš„Instruct/Prompt Tuningæ•°æ®
- https://zhuanlan.zhihu.com/p/615277009

### æ€»ç»“å½“ä¸‹å¯ç”¨çš„å¤§æ¨¡å‹LLMs
- https://zhuanlan.zhihu.com/p/611403556

### é’ˆå¯¹èŠå¤©å¯¹è¯æ•°æ®æ‘˜è¦ç”Ÿæˆä»»åŠ¡å¾®è°ƒ FLAN-T5
- https://www.philschmid.de/fine-tune-flan-t5

### ä½¿ç”¨ DeepSpeed å’Œ Hugging Face ğŸ¤— Transformer å¾®è°ƒ FLAN-T5 XL/XXL
- https://zhuanlan.zhihu.com/p/615528315

### ChatGPTç­‰å¤§æ¨¡å‹é«˜æ•ˆè°ƒå‚å¤§æ³•â€”â€”PEFTåº“çš„ç®—æ³•ç®€ä»‹
- https://zhuanlan.zhihu.com/p/613863520

### Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU
- https://huggingface.co/blog/trl-peft

### å¯ä»¥å¾®è°ƒç±»ChatGPTæ¨¡å‹å•¦ï¼å¼€æºAlpaca-LoRA+RTX 4090å°±èƒ½æå®š
- https://mp.weixin.qq.com/s/vzIm-fOxxPEU69ArAowoIg

### 0é—¨æ§›å…‹éš†ChatGPTï¼30åˆ†é’Ÿè®­å®Œï¼Œ60äº¿å‚æ•°æ€§èƒ½å ªæ¯”GPT-3.5
- https://mp.weixin.qq.com/s/RMrXIHGOy3cPu8ybQNWonA

### è®­ç»ƒä¸ªä¸­æ–‡ç‰ˆChatGPTæ²¡é‚£ä¹ˆéš¾ï¼šä¸ç”¨A100ï¼Œå¼€æºAlpaca-LoRA+RTX 4090å°±èƒ½æå®š
- https://mp.weixin.qq.com/s/k7T-vfoH3xvxl6uqImP7DQ

### GPT fine-tuneå®æˆ˜ï¼š è®­ç»ƒæˆ‘è‡ªå·±çš„ ChatGPT
- https://zhuanlan.zhihu.com/p/616504594

### ç¬”è®°æœ¬å°±èƒ½è¿è¡Œçš„ChatGPTå¹³æ›¿æ¥äº†ï¼Œé™„å®Œæ•´ç‰ˆæŠ€æœ¯æŠ¥å‘Š
- https://mp.weixin.qq.com/s/crpG4dtfQFe3Q7hR3oeyxQ

### ã€å®˜æ–¹æ•™ç¨‹ã€‘ChatGLM-6Bå¾®è°ƒï¼Œæœ€ä½åªéœ€7GBæ˜¾å­˜
- https://mp.weixin.qq.com/s/miML4PXioK5iM8UI0cTSCQ

### ç‰¹åˆ¶è‡ªå·±çš„ChatGPTï¼šå¤šæ¥å£ç»Ÿä¸€çš„è½»é‡çº§LLM-IFTå¹³å°
- https://mp.weixin.qq.com/s/Q5Q3RpQ80XmpbfhSxq2R1Q

### ChatDoctorï¼šåŸºäºLLaMAåœ¨åŒ»å­¦é¢†åŸŸçŸ¥è¯†ä¸Šå¾®è°ƒçš„åŒ»å­¦å¯¹è¯æ¨¡å‹
- https://mp.weixin.qq.com/s/-IqECOgCs4cS6Ya-EccXOA
- https://github.com/Kent0n-Li/ChatDoctor

### ä¹Ÿè°ˆChatGPTçš„ä½æˆæœ¬â€œå¹³æ›¿â€å½“ä¸‹å®ç°è·¯çº¿ï¼šè¯­è¨€æ¨¡å‹+æŒ‡ä»¤å¾®è°ƒæ•°æ®+å¾®è°ƒåŠ é€Ÿæ¶æ„ä¸‹çš„ä»£è¡¨é¡¹ç›®å’Œå¼€æ”¾æ•°æ®
- https://mp.weixin.qq.com/s/CJ4cCjti5jHOpDZqd42stw

### StackLLaMA: A hands-on guide to train LLaMA with RLHF
- https://huggingface.co/blog/stackllama

### æˆæœ¬ä¸åˆ°100ç¾å…ƒï¼UCä¼¯å…‹åˆ©å†å¼€æºç±»ChatGPTæ¨¡å‹ã€Œè€ƒæ‹‰ã€ï¼šæ•°æ®é‡å¤§æ²¡æœ‰ç”¨ï¼Œé«˜è´¨é‡æ‰æ˜¯ç‹é“
- https://zhuanlan.zhihu.com/p/621078208

### NLPå¤§æ¨¡å‹å¿…å¤‡-FudanNLPå¼€æºä¸­æ–‡å›¾ä¹¦é›†åˆCBook-150K
- https://mp.weixin.qq.com/s/X2SmjkALVVOE5hOrizcqqw
- https://github.com/FudanNLPLAB/CBook-150K
- http://www.doc-ai.cn/

### COIGï¼šé¦–ä¸ªå¤§è§„æ¨¡ã€å¯å•†ç”¨çš„ä¸­æ–‡å¼€æºæŒ‡ä»¤æ•°æ®ï¼
- https://mp.weixin.qq.com/s/1hSU5AROH0ZGuDo9oD0bFw
- https://huggingface.co/datasets/BAAI/COIG

### ä»¥ç«èµ›ä¸ºä¾‹--GPT/BART/CPTçš„é¢„è®­ç»ƒå’Œå¾®è°ƒå…¨æµç¨‹
- https://mp.weixin.qq.com/s/fNb9tmEXLUtDoWKibNFLEQ

### ç”Ÿæˆå¼ä¸“åˆ©è¯­è¨€æ¨¡å‹(PatentGPT)è¯„ä¼°
- https://mp.weixin.qq.com/s/hnmH8AzQupIZH1lWX2ZSNw

### æä½èµ„æºå¾®è°ƒå¤§æ¨¡å‹æ–¹æ³•LoRAä»¥åŠBLOOM-LORAå®ç°ä»£ç 
- https://zhuanlan.zhihu.com/p/625488835

### â€œè¶…è¶Šâ€(MMCU)ä¸­æ–‡é€šç”¨å¤§è¯­è¨€æ¨¡å‹æµ‹è¯•é›†--å›½å†…é¦–ä¸ªå¤šé¢†åŸŸå¤šä»»åŠ¡æ•°æ®é›†
- https://mp.weixin.qq.com/s/sZqqK51PamKHOz3DFcA_4A

æ•°æ®é›†çš„æµ‹è¯•å†…å®¹æ¶µç›–å››å¤§é¢†åŸŸï¼šåŒ»ç–—ã€æ³•å¾‹ã€å¿ƒç†å­¦å’Œæ•™è‚²ã€‚é€šè¿‡ç»¼åˆè¯„ä¼°æ¨¡å‹åœ¨å¤šä¸ªå­¦ç§‘ä¸Šçš„çŸ¥è¯†å¹¿åº¦å’Œæ·±åº¦ï¼Œèƒ½å¤Ÿå¸®åŠ©ç ”ç©¶è€…æ›´ç²¾å‡†åœ°æ‰¾å‡ºæ¨¡å‹çš„ç¼ºé™·ï¼Œå¹¶å¯¹æ¨¡å‹çš„èƒ½åŠ›è¿›è¡Œæ‰“åˆ†ã€‚

### CCKS2023-PromptCBLUEä¸­æ–‡åŒ»ç–—å¤§æ¨¡å‹è¯„æµ‹æ¯”èµ›
- https://mp.weixin.qq.com/s/LjOiZ_S7oLJBvqdKotA9zA

ä¸ºæ¨åŠ¨LLMåœ¨åŒ»ç–—é¢†åŸŸçš„å‘å±•å’Œè½åœ°ï¼Œåä¸œå¸ˆèŒƒå¤§å­¦è®¡ç®—æœºå­¦é™¢ç‹æ™“ç²æ•™æˆå›¢é˜Ÿè”åˆé˜¿é‡Œå·´å·´å¤©æ± å¹³å°ã€å¤æ—¦å¤§å­¦ã€å¤æ—¦å¤§å­¦é™„å±åå±±åŒ»é™¢ã€ä¸œåŒ—å¤§å­¦ã€å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦ï¼ˆæ·±åœ³ï¼‰ã€é¹åŸå®éªŒå®¤ä¸åŒæµå¤§å­¦æ¨å‡ºPromptCBLUEè¯„æµ‹åŸºå‡†(https://github.com/michael-wzhu/PromptCBLUE)ï¼Œå¯¹CBLUEåŸºå‡†(https://tianchi.aliyun.com/dataset/95414)è¿›è¡ŒäºŒæ¬¡å¼€å‘ï¼Œå°†16ç§ä¸åŒçš„åŒ»ç–—åœºæ™¯NLPä»»åŠ¡å…¨éƒ¨è½¬åŒ–ä¸ºåŸºäºæç¤ºçš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ï¼Œå½¢æˆé¦–ä¸ªä¸­æ–‡åŒ»ç–—åœºæ™¯çš„LLMè¯„æµ‹åŸºå‡†ã€‚PromptCBLUEå°†ä½œä¸ºCCKS-2023çš„è¯„æµ‹ä»»åŠ¡ä¹‹ä¸€ï¼Œå·²åœ¨é˜¿é‡Œå·´å·´å¤©æ± å¤§èµ›å¹³å°ä¸Šçº¿è¿›è¡Œå¼€æ”¾è¯„æµ‹ï¼Œæ¬¢è¿å„ä½å¸ˆç”ŸæŠ¥åå‚èµ›(åˆ·æ¦œ)ã€‚

### ä¹Ÿçœ‹å‚ç›´é¢†åŸŸå¤§æ¨¡å‹å¾®è°ƒè½åœ°-ä»¥åŒ»ç–—é¢†åŸŸä¸ºä¾‹ï¼šä»PMC-LLaMAå¢é‡é¢„è®­åˆ°MedicalGPT-zhæŒ‡ä»¤å¾®è°ƒé¡¹ç›®æ¦‚è¿°
- https://mp.weixin.qq.com/s/Pk4NdFQq5G6iZ3QmcyyFUg

### HuggingFaceå®£å¸ƒåœ¨transformersåº“ä¸­å¼•å…¥é¦–ä¸ªRNNæ¨¡å‹ï¼šRWKVï¼Œä¸€ä¸ªç»“åˆäº†RNNä¸TransformeråŒé‡ä¼˜ç‚¹çš„æ¨¡å‹
- https://zhuanlan.zhihu.com/p/629637598

### LLMè¯„ä»·æ¨¡å‹PandaLMæŠ€æœ¯å‰ç»
- https://zhuanlan.zhihu.com/p/630173415
- https://github.com/WeOpenML/PandaLM

### å°æ•°æ®ä¹Ÿèƒ½åŠ©åŠ›å¤§å‘ç°ï¼CancerGPTæˆåŠŸé¢„æµ‹è¯ç‰©ç»„åˆï¼ŒæƒŠäººæ•°å­—è¯æ˜å…¶å‡†ç¡®æ€§ï¼
- https://mp.weixin.qq.com/s/xswnXhnLOkVOQwfKNFdPQA

### é€æ­¥è’¸é¦ï¼ç”¨æ›´å°‘çš„æ•°æ®ï¼Œè®­ç»ƒæ›´å°çš„æ¨¡å‹ï¼šæ€§èƒ½å´å ªæ¯”å¤§2000å€çš„æ¨¡å‹
- https://mp.weixin.qq.com/s/dtKaeSO4hZPGOuPcHRmBQw

### å›½å†…é¦–ä¸ªå¯å¤ç°çš„ RLHF åŸºå‡†ï¼ŒåŒ—å¤§å›¢é˜Ÿå¼€æºPKU-Beaver | æ–™è§é—­é—¨äº¤æµ
- https://github.com/PKU-Alignment/safe-rlhf
- https://mp.weixin.qq.com/s/ZpkgszXbisl5xf63EfTNjQ

### Meta AI é‡ç£…æ¨å‡ºLIMAï¼åª²ç¾GPT-4ã€æ— éœ€RLHFå°±èƒ½å¯¹é½ï¼
- https://zhuanlan.zhihu.com/p/631508237

### é€¼è¿‘GPT-4ï¼BLOOMChat: å¼€æºå¯å•†ç”¨æ”¯æŒå¤šè¯­è¨€çš„å¤§è¯­è¨€æ¨¡å‹
- https://zhuanlan.zhihu.com/p/631036519

### æ‰‹æŠŠæ‰‹å¤ç°ä¸€ä¸ªChatGPT
- https://zhuanlan.zhihu.com/p/631690198

### å…³äºhippocratic.aiå’Œglass.healthçš„äº§å“è®¨è®º
- https://mp.weixin.qq.com/s/yl_aPKg74yHKNdfPhGss5g

### è¶Šå°è¶Šå¥½: Q8-Chatï¼Œåœ¨è‹±ç‰¹å°”è‡³å¼º CPU ä¸Šä½“éªŒé«˜æ•ˆçš„ç”Ÿæˆå¼ AI
- https://mp.weixin.qq.com/s/O55qgGeD5lDKl9tGVmBN3g

### * å¼€æºåŸé©¼ï¼ˆGuanacoï¼‰åŠèƒŒåçš„QLoRAæŠ€æœ¯ï¼Œå°†å¾®è°ƒ65Bæ¨¡å‹çš„æ˜¾å­˜éœ€æ±‚ä»780GBä»¥ä¸Šé™ä½åˆ°48GBä»¥ä¸‹ï¼Œæ•ˆæœç›´é€¼GPT-4ï¼ŒæŠ€æœ¯è¯¦è§£
- https://zhuanlan.zhihu.com/p/632236718

### * ä½¿ç”¨LoRAå¯¹BELLEå‘å¸ƒçš„BELLE-7B-2Mè¿›è¡Œå¾®è°ƒ
- https://zhuanlan.zhihu.com/p/632317500

### * ã€LLMç³»åˆ—ä¹‹Tokenizerã€‘å¦‚ä½•ç§‘å­¦åœ°è®­ç»ƒä¸€ä¸ªLLMåˆ†è¯å™¨
- https://mp.weixin.qq.com/s/z6wUY1p8_AVv8YEQ6FRYIA

### * é‡‘èé¢†åŸŸå¤§æ¨¡å‹æ•ˆæœï¼Œä½æˆæœ¬ï¼ŒJust-in-Timeï¼Œåœºæ™¯è½åœ°
- https://mp.weixin.qq.com/s/5Nm1I10eLi0xhNIxqyEOMA

### * é¦–ä¸ªå¤§è§„æ¨¡ä½¿ç”¨å·¥å…·çš„å¤§æ¨¡å‹æ¥äº†ï¼šä¼¯å…‹åˆ©å‘å¸ƒGorilla
- https://mp.weixin.qq.com/s/p9tx3q3Lpr4fNqdyxWhzyA

### * NBCEï¼šä½¿ç”¨æœ´ç´ è´å¶æ–¯æ‰©å±•LLMçš„Contextå¤„ç†é•¿åº¦
- https://kexue.fm/archives/9617

> æŒç»­æ›´æ–°ä¸­ (Continuously Updated)... 

